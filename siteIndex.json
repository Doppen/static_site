[{"id":0,"title":"Home","body":"Heading two\n\nthis is the content of the markdown file.","ref":"index.html"},{"id":1,"title":"Expertise","body":"We support textual research on four levels. The first one is close reading and manual annotation. The second one is advanced search of annotated text collections. The third one is statistical methods (machine learning, text mining). And the last one is deriving structured data from annotated text. Our Team Lead is Hennie Brugman.\n\nWe work on software that presents, searches, annotates and analyses text. Our goal is to create a common pipeline, enabling scholars and engineers to ingest, process and publish textual data – be it XML, raw text, or scans – in a common distributed and modular infrastructure adaptable to the different scholarly domains at the HuC and outside.\n\n\n\nText analysis is a large part of the responsibilities of our team. The focus lies on tools for linguistic, syntactic and semantic analysis, NER, as well as other information extraction algorithms. The total suite of tooling should automatically generate a context of metadata and annotations around a text, and enable users to confirm, reject or correct these annotations. The work of the team in this field interacts closely with more experimental development in R&amp;D, at the DHLab and various research groups in computational science and computational linguistics. Prototypes from these groups can be adopted by the team if they can be improved to a certain level of maturity.\n\nWe are also responsible for packaging products and product components into interactive environments that are optimised for the specific needs of researchers or research projects.\n\nTeam\n\nHennie Brugman (Team Lead) (Publications)Bram Buitendijk (Publications)Hayco de Jong\n\nProduct groups\n\nWe work on products in the following product groups:\n\nGeneric but flexible front end solutions: we try to exploit shared functionality between user interfaces while maintaining the flexibility to meet special project requirements.Tooling for several traditional and more innovative ways to annotate, and exploit these annotations.Searching in large text collections in ways that list/summarize/organize/visualize large amounts of hits.A state of the art text repository that supports the full life cycle of text documents in all their forms and versions, and in relation to all associated annotations and enrichments.Pipelines and tools for analysing, enriching and processing text documents.\n\nSample Projects\n\nCLARIAH Plus, creates a national digital infrastructure for the humanities.EviDENce studies how eyewitnesses have reported on violence, and how this may have changed over time.HiTimeP, is an annotation interface for entity linking.REPUBLIC, will provide access to the resolutions of the States General (1576-1796): more than half a million pages with handwritten and printed political information.\n\nCode repository\n\nGitHub\n\nContact\n\nhennie.brugman@di.huc.knaw.nl","ref":"text-analysis.html"},{"id":2,"title":"Text Analysis","body":"We support textual research on four levels. The first one is close reading and manual annotation. The second one is advanced search of annotated text collections. The third one is statistical methods (machine learning, text mining). And the last one is deriving structured data from annotated text. Our Team Lead is Hennie Brugman.\n\nWe work on software that presents, searches, annotates and analyses text. Our goal is to create a common pipeline, enabling scholars and engineers to ingest, process and publish textual data – be it XML, raw text, or scans – in a common distributed and modular infrastructure adaptable to the different scholarly domains at the HuC and outside.\n\n\n\nText analysis is a large part of the responsibilities of our team. The focus lies on tools for linguistic, syntactic and semantic analysis, NER, as well as other information extraction algorithms. The total suite of tooling should automatically generate a context of metadata and annotations around a text, and enable users to confirm, reject or correct these annotations. The work of the team in this field interacts closely with more experimental development in R&amp;D, at the DHLab and various research groups in computational science and computational linguistics. Prototypes from these groups can be adopted by the team if they can be improved to a certain level of maturity.\n\nWe are also responsible for packaging products and product components into interactive environments that are optimised for the specific needs of researchers or research projects.\n\nTeam\n\nHennie Brugman (Team Lead) (Publications)Bram Buitendijk (Publications)Hayco de Jong\n\nProduct groups\n\nWe work on products in the following product groups:\n\nGeneric but flexible front end solutions: we try to exploit shared functionality between user interfaces while maintaining the flexibility to meet special project requirements.Tooling for several traditional and more innovative ways to annotate, and exploit these annotations.Searching in large text collections in ways that list/summarize/organize/visualize large amounts of hits.A state of the art text repository that supports the full life cycle of text documents in all their forms and versions, and in relation to all associated annotations and enrichments.Pipelines and tools for analysing, enriching and processing text documents.\n\nSample Projects\n\nCLARIAH Plus, creates a national digital infrastructure for the humanities.EviDENce studies how eyewitnesses have reported on violence, and how this may have changed over time.HiTimeP, is an annotation interface for entity linking.REPUBLIC, will provide access to the resolutions of the States General (1576-1796): more than half a million pages with handwritten and printed political information.\n\nCode repository\n\nGitHub\n\nContact\n\nhennie.brugman@di.huc.knaw.nl","ref":"text-analysis.html"},{"id":3,"title":"Structured Data","body":"We build software that ingests, curates, stores, queries and displays structured data that can help researchers answer questions about entities such as historical persons, places, objects, and events or survey linguistic phenomena. Our Team Lead is Menzo Windhouwer.\n\nStructured Data builds software that provides users with insight into datasets. Software that ingests, curates, stores, queries, and displays structured data with the goal of creating a knowledge graph that, for example, can answer ‘who’, ‘what’, ‘where’, and ‘when’ questions.\n\nThe team develops tools for managing data for various formats. In addition to applications with a graphical interface, the SD team offers infrastructure with programming interfaces (APIs) such as GraphQL. This allows us to build custom applications, offer reconciliation and alignment methods to curate datasets, and archive data.\n\nTeam\n\nKerim Meijer  (Publications)Maarten van der Peet (Publications)Meindert Kroese (Publications)Menzo Windhouwer (Team Lead) (Publications)Rob Zeeman (Publications)\n\nProducts\n\nTimbuctoo RDF store helps you get insights on your RDF dataset.Reconciliation tools to align the data in your Timbuctoo dataset.CMDI editor simplifies editing CMDI documentsKabara enables you to run SPARQL queries on your Timbcutoo dataset.\n\nSample Projects\n\nCLARIAH, a distributed infrastructure for the humanities and social sciences.CLARIAH Plus, creates a national digital infrastructure for the humanities.Golden Agents will connect scattered heterogeneous sources (new and existing) about creative industries in the Golden Age via Linked Open Data.\n\nCode Repository\n\nGitHub\n\nContact\n\nstructured-data@di.huc.knaw.nl","ref":"structured-data.html"},{"id":4,"title":"Computer Vision","body":"We process large volumes of digital photographs and page images and generate new data from them to address tasks such as text recognition, handwriting transcription, author identification, dating, localization, and the analysis of paper and manuscripts. Our Team Lead is Rutger van Koert.\n\nTeam\n\nMartijn Maas (Publications)Rutger van Koert (Team Lead) (Publications)\n\nSample Projects\n\nREPUBLIC will provide access to the resolutions of the States General (1576-1796): more than half a million pages with handwritten and printed political information.Digital Forensics explores new ways to use digital image analysis to analyse historical script samples.Dutch Prize Papers makes 144.000 digitised pages from highly varied documents accessible to researchers and interested parties in a structured research environment.TRIADO develops an efficient and effective methodology for converting large amounts of unstructured, analogue data from archive collections into usable digital research data.\n\nRead more\n\nRutger van Koert, Hannah Busch and Mariken Teeuwen, Cutting up medieval manuscripts (digitally) to trace their origins\n\nCode Repositories\n\nGitHub\n\nContact\n\nrutger.van.koert@di.huc.knaw.nl","ref":"computer-vision.html"},{"id":5,"title":"Spatial Computing","body":"We build tools to extract geographical shapes, such as national borders, and entities from data, to compare historical maps more easily and analyse the new data in different ways. Thanks to the availability of new data sets and the development of new technologies, the possibilities for comparing data in time and space are endless. Within the Humanities and Social Sciences, there is a great demand for a reliable infrastructure to map and visualise data geographically.\n\nTeam\n\nThomas Vermaut (Team Lead) (Publications)\n\nProjects\n\nThe HisGIS project links historical information to digitised maps. His stands for historical and GIS for Geographical Information System. With HisGIS you can time travel through the past of the Netherlands. HisGIS is set up by region and by city. Most provinces and a number of large cities have already been mapped. In time, the system must cover the whole of the Netherlands. Because all files are freely available for research, the various Time Machines in the Netherlands also build on HisGIS. The maps and Time Machines are also accessible for general use via a public viewer on www.hisgis.nl. HisGIS makes new infrastructure available within CLARIAH Plus and Time Machine.\n\nCode repository\n\nGitHub","ref":"spatial-computing.html"},{"id":6,"title":"Interface Design","body":"Humanities data is often complex and hard to interpret. Our team focuses on making this data accessible and understandable. The output of the KNAW Humanties Cluster must be accessible to many people, not just to researchers. We create tools that places data in the right context so that they are better understood and easy to use. And we strive to make our interfaces inclusive, device-agnostic and long-lasting so that these can be accessible long into the future. The team specialises in user research, web usability, interface design, web publication, data visualisation and project branding.\n\nTeam\n\nBas Doppen (design officer) (Publications)\n\nProjects\n\nGolden Agents will connect scattered heterogeneous sources (new and existing) about creative industries in the Golden Age via linked open data.Dutch Prize Papers makes 144.000 digitised pages from highly varied documents accessible to researchers and interested parties in a structured research environment.Timbuctoo is specifically designed for academic research in the Arts and Humanities, which often yields complex and heterogeneous data.Rooswijk is a virtual exhibition about the VOC ship of the same name that perished in 1740 off the English coast.\n\nCode Repository\n\nGitHub\n\nContact\n\nbas.doppen@di.huc.knaw.nl","ref":"interface-design.html"},{"id":7,"title":"Cloud Computing","body":"We are responsible for the core infrastructure used throughout the department. Our core tasks are management of computing resources, long-term maintenance and development of our production platforms, and running software that is no longer under development. Our Team Lead is Mario Mieldijk.\n\nThe infrastructure in the KNAW Humanities Cluster consists of approximately 200 ‘virtual’ servers, 100 ‘physical’ servers, more than 1 Petabyte (1,000,000 GB) of digital storage and approximately 380 company-critical applications, websites, and web applications.\n\nTeam Concern Infrastructure is working on an infrastructure for research projects and collections, that is as stable, safe, and fast as possible. The team develops user-friendly systems so that functional managers can do as much as possible themselves. The team supplies and manages many of its resources via a PaaS (Platform as a Service). Some examples include Kubernetes, Docker, vmWare, and Database clusters. The team delivers, manages and optimizes business-critical applications and workflows such as Web hosting, Archive systems and Library systems. Applications whose further development has ended are maintained by our team. In addition, we perform important tasks in information security.\n\nCode Repository\n\nGitHub\n\nContact\n\nconcern-infrastructure@di.huc.knaw.nl","ref":"cloud-computing.html"},{"id":8,"title":"Activities","body":"Heading two\n\nthis is the content of the markdown file.","ref":"research-projects.html"},{"id":9,"title":"Research Projects","body":"Heading two\n\nthis is the content of the markdown file.","ref":"research-projects.html"},{"id":10,"title":"Infrastructure Projects","body":"Heading two\n\nthis is the content of the markdown file.","ref":"infrastructure-projects.html"},{"id":11,"title":"Sustainability","body":"Heading two\n\nthis is the content of the markdown file.","ref":"sustainability.html"},{"id":12,"title":"Software & Datasets","body":"Heading two\n\nthis is the content of the markdown file.","ref":"software---datasets.html"},{"id":13,"title":"Working with DI","body":"Heading two\n\nthis is the content of the markdown file.","ref":"working-with-di.html"},{"id":14,"title":"Partners","body":"Heading two\n\nthis is the content of the markdown file.","ref":"knaw-research-institutes.html"},{"id":15,"title":"KNAW Research Institutes","body":"Heading two\n\nthis is the content of the markdown file.","ref":"knaw-research-institutes.html"},{"id":16,"title":"NL-Lab & DHLab","body":"Heading two\n\nthis is the content of the markdown file.","ref":"nl-lab---dhlab.html"},{"id":17,"title":"Cultural AI Lab","body":"Heading two\n\nthis is the content of the markdown file.","ref":"cultural-ai-lab.html"},{"id":18,"title":"External Partners","body":"Heading two\n\nthis is the content of the markdown file.","ref":"external-partners.html"},{"id":19,"title":"About","body":"Heading two\n\nthis is the content of the markdown file.","ref":"mission.html"},{"id":20,"title":"Mission","body":"Heading two\n\nthis is the content of the markdown file.","ref":"mission.html"},{"id":21,"title":"Staff","body":"Heading two\n\nthis is the content of the markdown file.","ref":"staff.html"},{"id":22,"title":"News","body":"Heading two\n\nthis is the content of the markdown file.","ref":"news.html"},{"id":23,"title":"Events","body":"Heading two\n\nthis is the content of the markdown file.","ref":"events.html"},{"id":24,"title":"Contact","body":"Heading two\n\nthis is the content of the markdown file.","ref":"contact.html"},{"id":25,"title":"(Re)counting the Uncounted","body":"Rombert Stapel van het IISG koppelt bronnen zoals huis- en haardtellingen aan een GIS-dataset. Dat moet resulteren in een methode die vergeleken met bestaande schattingen veel nauwkeurigere bevolkingsschattingen oplevert. Hiermee wordt toekomstig onderzoek betrouwbaarder.\n\nBetrouwbare schattingen van de omvang van de bevolking zijn essentiële bouwstenen voor historisch, economisch en sociaal wetenschappelijk onderzoek. Dat geldt al helemaal voor een sterk verstedelijkt gebied als de Lage Landen, dat de ‘eerste moderne economie’ mogelijk maakte. Rombert Stapel zal in zijn onderzoek ‘(Re)counting the Uncounted. Replication and Contextualisation of Dutch and Belgian Premodern Population Estimates (1350-1800)’ een koppeling maken tussen huis- en haardtellingen per woonplaats en de GIS dataset &#39;Historical Atlas of the Low Countries&#39; die op het IISG wordt ontwikkeld. Door deze koppeling wordt het mogelijk veel nauwkeurigere en minder ondoorzichtige bevolkingsschattingen te maken dan voorheen.\n\nRead More\n\nDat maakt dit onderzoek zo veel waard [...] Deze nieuwe methode zal een verrijking zijn voor heel veel typen nieuw onderzoek.\n\n  Rombert Stapel, IISG","ref":"re-counting-uncounted.html"},{"id":26,"title":"Another featured project","body":"Rombert Stapel van het IISG koppelt bronnen zoals huis- en haardtellingen aan een GIS-dataset. Dat moet resulteren in een methode die vergeleken met bestaande schattingen veel nauwkeurigere bevolkingsschattingen oplevert. Hiermee wordt toekomstig onderzoek betrouwbaarder.\n\nBetrouwbare schattingen van de omvang van de bevolking zijn essentiële bouwstenen voor historisch, economisch en sociaal wetenschappelijk onderzoek. Dat geldt al helemaal voor een sterk verstedelijkt gebied als de Lage Landen, dat de ‘eerste moderne economie’ mogelijk maakte. Rombert Stapel zal in zijn onderzoek ‘(Re)counting the Uncounted. Replication and Contextualisation of Dutch and Belgian Premodern Population Estimates (1350-1800)’ een koppeling maken tussen huis- en haardtellingen per woonplaats en de GIS dataset &#39;Historical Atlas of the Low Countries&#39; die op het IISG wordt ontwikkeld. Door deze koppeling wordt het mogelijk veel nauwkeurigere en minder ondoorzichtige bevolkingsschattingen te maken dan voorheen.\n\nRead More\n\nDat maakt dit onderzoek zo veel waard [...] Deze nieuwe methode zal een verrijking zijn voor heel veel typen nieuw onderzoek.\n\n  Rombert Stapel, IISG","ref":"afp.html"},{"id":27,"title":"News item one","body":"Rombert Stapel van het IISG koppelt bronnen zoals huis- en haardtellingen aan een GIS-dataset. Dat moet resulteren in een methode die vergeleken met bestaande schattingen veel nauwkeurigere bevolkingsschattingen oplevert. Hiermee wordt toekomstig onderzoek betrouwbaarder.\n\nBetrouwbare schattingen van de omvang van de bevolking zijn essentiële bouwstenen voor historisch, economisch en sociaal wetenschappelijk onderzoek. Dat geldt al helemaal voor een sterk verstedelijkt gebied als de Lage Landen, dat de ‘eerste moderne economie’ mogelijk maakte. Rombert Stapel zal in zijn onderzoek ‘(Re)counting the Uncounted. Replication and Contextualisation of Dutch and Belgian Premodern Population Estimates (1350-1800)’ een koppeling maken tussen huis- en haardtellingen per woonplaats en de GIS dataset &#39;Historical Atlas of the Low Countries&#39; die op het IISG wordt ontwikkeld. Door deze koppeling wordt het mogelijk veel nauwkeurigere en minder ondoorzichtige bevolkingsschattingen te maken dan voorheen.\n\nRead More\n\nDat maakt dit onderzoek zo veel waard [...] Deze nieuwe methode zal een verrijking zijn voor heel veel typen nieuw onderzoek.\n\n  Rombert Stapel, IISG","ref":"news-item-one.html"}]